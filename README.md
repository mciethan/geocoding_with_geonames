## Background
As a GIS and Data Associate working in the Brown library, I was asked to explore geocoding options for the Stolen Relations project (https://indigenousslavery.org/), which seeks to document histories of enslavement of indigenous peoples in the Americas.  People associated with this project had scoured the archives and assembled a table with thousands of records of indigenous enslavement spanning several centuries.  One of the columns in this table listed location information associated with each record, and they were looking to plot each record as a point on the map by identifying coordinates for each place name.

## Rationale for not using a traditional geocoding service
Several aspects of the Stolen Relations dataset made it challenging to achieve good results using traditional geocoding services.  Each location had an indeterminate number of components, the level of spatial detail ranged from individual buildings to entire countries, and everything was in a single column rather than being split in some way to identify different components of each place name.  Many of the place names themselves were colonial names that are no longer in use.  And in some cases, what was listed in the location column was not a single place name with ordered components, but a collection of distinct place names that were all mentioned in a particular historical record.  

Running the original dataset through the ArcGIS World Geocoding Service resulted in hundreds of incorrectly plotted points as well as missing points.  Doing some pre-processing to identify and split country, state, or city names into their own columns and running that through ArcGIS's geocoding service reduced some (but not most) of the false matches, and still failed to plot points for hundreds of records.

## High-level notes on how this script works
This script represents my first attempt at implementing a custom geocoding algorithm, and although it may have some utility for geocoding messy data beyond the Stolen Relations project, its features are tailored for the Stolen Relations data in certain ways.  In particular, with the Stolen Relations data, I could more or less rely on each place name having its components separated with commas, even if I didn't know which components they were.  

At the most basic level, the script uses a for loop to iterate through all the place names in the dataset.  Within the for loop, there is a while loop that iterates through the components of each place name.  Inside this while loop, there is a large if-block which can be thought of as a decision tree.  To traverse this tree, we update local variables that send the program down different branches of the tree on each new iteration of the while loop.  I drew a schematic diagram (stolen_relations_script_diagram.png) to try and visualize the logic behind this tree of dictionaries.  I have also tried to leave ample inline comments within the script itself explaining how everything works.

## Sources of data
For each place name, the script first searches for the name within a series of hashtables containing coordinate data for world countries and cities as well as US states and counties.  I used public-domain data for the country, state, and county coordinates.  For the coordinates of world cities, I downloaded a free dataset from https://simplemaps.com/data/world-cities, which is in turn based on data from sources like the US Census, the USGS, and NASA.

To handle place names that don't show up in these standard lists of place names, I decided to use the Search Webservice API offered by GeoNames (https://www.geonames.org/export/geonames-search.html), which supports open-ended or parameterized searches for place names and returns coordinates along with other information.  GeoNames sets hourly, daily, and weekly limits on the number of requests that will be fulfilled for a given username, so to prevent redundant searches and avoid hitting my limits, I set up a dictionary and a local csv file to store information from previous GeoNames searches, such that I wouldn't do the same search twice.  

## Geocoding performance and accuracy
Overall, the script took around 3 minutes to execute the 600-ish requests to GeoNames that it needed to process all 3000+ rows of the original data.  Because the script stores all the data it pulls from the requests to be reused later, subsequent script runs took only a few seconds each, since they didn't involve any GeoNames requests, and since all the data are stored in dictionaries for constant-time lookup rather than in lists or other structures that need to be searched.  

The script failed to retrieve coordinates for only 10 of the original records, which I remedied without much pain by manually adding some information to those records.  Plotting the results on a map, I could not find any points that had been plotted in the wrong country or the wrong US state.  Most of the points were additionally accurate down to the subregion, city, or town.  
